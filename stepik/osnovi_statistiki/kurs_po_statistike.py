import math
import statistics
from icecream import ic
import matplotlib.pyplot as plt


some_list1 = [1, 2, 3, 4, 5, 6, 7, 8, 9]
some_list2 = [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]
# Среднее значение - μ (мю)
average1 = statistics.mean(some_list1)
average2 = statistics.mean(some_list2)
x1 = some_list1[5]
x2 = some_list2[5]

# Дисперсия - средний квадрат отклонений индивидуальных значений признака от их средней величины


def find_variance(list):
    d = 0
    average = statistics.mean(list)
    for num in list:
        d += (num - average)**2
    # -1 для выборки; для генеральной совокупности - число элементов
    d = d / (len(list) - 1)
    return d


d1 = find_variance(some_list1)
d2 = find_variance(some_list2)

# Стандартное отклонение (σ) - корень из дисперсии
# 68% всех элементов лежат в диапазоне +- 1 сигма, 95% +- 2 сигмы, 99% +- 3 сигмы
sd1 = math.sqrt(d1)
sd2 = math.sqrt(d2)

# Z-преобразование
xz1 = (x1 - average1) / sd1
xz2 = (x2 - average2) / sd2

# Стандартная ошибка среднего
se1 = sd1 / math.sqrt(len(some_list1))
se2 = sd2 / math.sqrt(len(some_list2))

# Распределение Стьюдента (t-распределение). Применяется, когда число наблюдений невелико и
# стандартное отклонение генеральной совокупности неизвестно. Важный параметр, определяющий форму
# распределения - степень свободы (df = n - 1). Чем больше df, тем ближе распределение к нормальному.

# Обратите внимание, что для расчета стандартной ошибки мы используем именно стандартное отклонение
# в генеральной совокупности - σ. Ранее мы уже обсуждали, что на практике σ нам практически никогда
# неизвестна, и для расчета стандартной ошибки мы используем выборочное стандартное отклонение.

# Так вот, строго говоря в таком случае распределение отклонения выборочного среднего и среднего в
# генеральной совокупности, деленного на стандартную ошибку, теперь будет описываться именно при
# помощи t - распределения.
t = (x1 - average1) / se1

# t-критерий Стьюдента - позволяет сравнить между собой два выборочных средних.
t_test = ((x1 - x2) - (average1 - average2)) / \
    math.sqrt(((sd1**2) / len(some_list1)) + ((sd2**2) / len(some_list2)))
df_sum = len(some_list1) + len(some_list2) - 2

# ic(len(some_list1))
# ic(d1)
# ic(sd1)
# ic(xz1)
# ic(se1)
# ic(t)
# ic(t_test)
# ic(df_sum)

# Задача 2.2.12
x = 89.9
sd = 11.3
n = 20
df = n - 1
se = (sd / math.sqrt(n)) * 2.093
x11 = x + se
x12 = x - se
# ic(se)
# ic(x11)
# ic(x12)

# Задача 2.2.13
xm = 45
sdm = 9
nm = 1000
xw = 34
sdw = 10
nw = 100
df_sum = nm + nw - 2
t_test = (xm - xw) / math.sqrt(((sdm**2) / nm) + ((sdw**2) / nw))
# ic(df_sum)
# ic(t_test)

# TODO Тест Шапиро-Вилка
# TODO U-критерий Манна-Уитни

# Однофакторный дисперсионный анализ. m - число групп.
m = 3

list1 = [3, 1, 2]
list1_avg = statistics.mean(list1)
list2 = [5, 3, 4]
list2_avg = statistics.mean(list2)
list3 = [7, 6, 5]
list3_avg = statistics.mean(list3)
x_avg = (list1[0] + list1[1] + list1[2] +
         list2[0] + list2[1] + list2[2] +
         list3[0] + list3[1] + list3[2]) / (len(list1) + len(list2) + len(list3))
# Общая изменчивость данных (сумма квадратов).
# Подразделяется на сумму квадратов межгрупповую (SSB) и сумму квадратов внутригрупповая (SSW)
sst = (list1[0] - x_avg)**2 + (list1[1] - x_avg)**2 + (list1[2] - x_avg)**2 + (list2[0] - x_avg)**2 + (list2[1] -
                                                                                                       x_avg)**2 + (list2[2] - x_avg)**2 + (list3[0] - x_avg)**2 + (list3[1] - x_avg)**2 + (list3[2] - x_avg)**2
df_t = len(list1) + len(list2) + len(list3) - 1

ssw = (list1[0] - list1_avg)**2 + (list1[1] - list1_avg)**2 + (list1[2] - list1_avg)**2 + (list2[0] - list2_avg)**2 + (list2[1] -
                                                                                                                       list2_avg)**2 + (list2[2] - list2_avg)**2 + (list3[0] - list3_avg)**2 + (list3[1] - list3_avg)**2 + (list3[2] - list3_avg)**2
df_w = len(list1) + len(list2) + len(list3) - m

ssb = len(list1) * (list1_avg - x_avg)**2 + len(list2) * \
    (list2_avg - x_avg)**2 + len(list3) * (list3_avg - x_avg)**2
df_b = m - 1

# sst = ssw + ssb. Если ssb > ssw, то группы значительно отличаются между собой.
# В противном случае большая часть имеющейся изменчивости обеспечивается внутри группы.

f = (ssb / df_b) / (ssw / df_w)

# ic(x_avg)
# ic(sst)
# ic(df_t)
# ic(ssw)
# ic(df_w)
# ic(ssb)
# ic(df_b)
# ic(f)

# Множественные сравнения. Поправка Бонферрони - чтобы удержать требуемый р-уровень значимости,
# необходимо его разделить на (кол-во групп * (кол-во групп - 1) / 2)

# TODO Критерий Тьюки

# Корреляция - взаимосвязь двух количественных переменных.
# (xi - x_avg) * (yi - y_avg) > 0 - Положительная взаимосвязь (первая и третья четверти
# относительно среднего значения х и у). Если < 0 - отрицательная взаимосвязь (2 и 4 четверти)
# Коэффициент ковариации (cov) -  среднее от суммы произведений отклонений от средних.
# cov = (sum(xi - x_avg) * (yi - y_avg)) / (n - 1)
# Коэффициент корреляции (r) - отношение ковариации к произведению стандартных отклонений.
# r = cov / (sdx * sdy), изменяется в диапазоне от -1 до 1
# Если r = 0, то переменные не связаны между собой
# r = sum((xi - x_avg) * (yi - y_avg)) / (sum(xi - x_avg)**2 * sum(yi - y_avg)**2)**0.5
# Коэффициент детерминации = квадрату корреляции. Часть изменчивости одной переменной, которая
# обусловлена тем, что на нее влияет другая переменная.

x = [4, 5, 2, 3, 1]
y = [2, 1, 4, 3, 5]
dx = find_variance(x)
dy = find_variance(y)
sdx = dx ** 0.5
sdy = dy ** 0.5


def find_cov(x, y):
    sum = 0
    for i in range(len(x)):
        sum += ((x[i] - statistics.mean(x)) * (y[i] - statistics.mean(y)))
    return sum / (len(x) - 1)


cov = find_cov(x, y)
r = cov / (sdx * sdy)

# ic(sdx, sdy)
# ic(cov)
# ic(r)
# plot = plt.plot(x, y)
# plt.savefig(fname='r_example')

# Коэффициент корреляции Пирсона применим при линейной взаимосвязи.
# Он чувствителен к нормальности распределения переменных и выбросам.

# Коэффициент корреляции Спирмена
# Аналог критерия Манна-Уитни. Переход к ранжированным значениям.
# rs = 1 - ((6 * sum(d**2)) / (N * (N**2 - 1)))
# d**2 - разность между рангами в квадрате

# Ошибка корреляции - положительная или отрицательная взаимосвязь
# еще не позволяют сделать вывод о причинно-следственной зависимости.

# Регрессия с одной независимой переменной (предиктор, x) - как одна переменная
# определяет, помогает предсказать вторую переменную
# y = b0 + b1 * x, где b0 - intercept (где прямая пересекает ось y)
#                      b1 - определяет направление и угол наклона прямой

# Метод наименьших квадратов - метод нахождения оптимальных параметров линейной
# регрессии, таких, что сумма квадратов ошибок (остатков) была минимальна
# Остаток (е) - отклонение реального значения от предсказанного регрессионой прямой
# Качество регрессионной прямой можно оценить суммой квадратов остатков
# Чем меньше, тем лучше она отражает взаимосвязь
# b1 = sdy / sdx * r
# b0 = y_avg - b1 * x_avg

x_avg = 15
dx = 25
y_avg = 10
dy = 36
sdx = dx ** 0.5
sdy = dy ** 0.5
det = 0.25
r = det ** 0.5
b1 = sdy / sdx * r

# ic(b1)

# Гипотеза о значимости взаимосвязи и коэффициент детерминации
# Проверка гипотезы осуществляется с применением t-критерия
# t = (b1 - 0) / se
# t-критерий проверяет гипотезу о том, что b1 отличен от нуля, т.е. зависимая
# переменная оказывает влияние на зависимую.

# Коэффициент детерминации (R**2) - доля дисперсии зависимой переменной, объясняемая
# регрессионной моделью
# R**2 = 1 - ss_res / ss_total, где ss_res - сумма квадратов остатков
#                                   ss_total - общая сумма квадратов (расстояние от
#                                              наблюдения до среднего значения)

# Условия применения линейной регрессии с одним предиктором
# 1) Линейная взаимосвязь x и y
# 2) Нормальное распределение остатков
# 3) Гомоскедастичность - постоянная изменчивость остатков на всех уровнях независимой
# переменной

# Регрессионный анализ с несколькими независимыми переменными
# Множественная регрессия позволяет исследовать влияние сразу нескольких независимых
# переменных на одну зависимую.
# y = b0 + b1 * x1 + ... + bn * xn

# Требования к данным:
# 1) Линейная зависимость переменных
# 2) Нормальное распределение остатков
# 3) Гетероскедастичность
# 4) Проверка на мультиколлинеарность
# 5) Нормальное респределение переменных (желательно)
# Исправленный R-квадрат - скорректированный коэффициент детерминации. Рассчитывается
# при включении в модель дополнительных независимых переменных.
# Множественная регрессия позволяет увидеть влияние отдельных переменных при учете,
# что остальные из них фиксированы

# receipts = 7,68 + 3,66*cost + 7,62*promotion + 0,82*books
receipts = 150
cost = 10
books = 8
promotion = (receipts - 7.68 - 3.66*cost - 0.82*books) / 7.62
# ic(promotion)

# Выбор наилучшей модели
# Мультиколлинеарность - очень сильная взаимосвязь между какими-то независимыми
# переменными. Оказывает негативное влияние на эффективонсть модели.

# Эффективность модели определяется R-квадратом. Не всегда целесообразно включать
# все имеющиеся переменные в регрессионную модель.
# Следует оценить корреляцию между каждой парой независимых переменных, чтобы
# оценить какую из них возможно исключить с целью увеличения эффективности модели.
# Возможно включение всех независимых переменных в уравнение модели и попеременное
# исключение и последующий анализ влияния на R-квадрат.

hs_grad = 90
metro_res = 80
white = 50
povetry = 68.7 - 0.06 * white - 0.05 * metro_res - 0.57 * hs_grad
# ic(povetry)

# Классификация: логистическая регрессия и кластерный анализ
